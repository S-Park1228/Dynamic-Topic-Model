{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct the input data from the preprocessed text data to apply to gensim\n",
    "- Inputs for gensim: time_slice, dictionary, corpus\n",
    "- Set the range for the hypothetical number of topics and then run Dynamic Topic Model\n",
    "- Determine the number of topics based on the coherence scores\n",
    "- Visualization and analysis of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constructing the input data to apply to gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the preprocessed news data\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "with open(\"data/common.pk\", \"rb\") as f:\n",
    "    input_data = pickle.load(f)\n",
    "\n",
    "input_data.reset_index(drop = True, inplace = True)\n",
    "print(input_data.head())\n",
    "print(input_data.tail())\n",
    "print(input_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting time_slice \n",
    "time_slice = [len(items) for items in input_data['title']]\n",
    "time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the tokenized news_contents as a list\n",
    "tokenized_data = []\n",
    "\n",
    "for i in range(0, len(input_data['contents']), 1):\n",
    "    \n",
    "    for j in range(0, time_slice[i], 1):\n",
    "        \n",
    "        a = input_data['contents'][i][j]\n",
    "        tokenized_data.append(a)\n",
    "        \n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_sum(arr, a):\n",
    "    arr = [0] + arr\n",
    "    partial_sum = [0] * len(arr)\n",
    "    \n",
    "    for i in range(1, len(arr)):\n",
    "        partial_sum[i] = partial_sum[i-1] + arr[i]\n",
    "        \n",
    "    partial_sum = partial_sum[1:]\n",
    "    # print(\"partial_sum\", partial_sum)\n",
    "    \n",
    "    # print(\"total sum\", partial_sum[-1])\n",
    "    \n",
    "    return partial_sum[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if tokenized_data is correctly organized by comparing the recent time.\n",
    "k = len(time_slice) - 2\n",
    "tokenized_data[cumulative_sum(time_slice, k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intall gensim.\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading classes to construct dictionary and corpus along with the gensim library. \n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "import os\n",
    "\n",
    "# Warning: If you run another model, you have to change the red-colored file name below.\n",
    "# Otherwise you will lose the original dictionary and corpus.\n",
    "# Construct dictionary.\n",
    "if not os.path.exists('common(DTM)_dict'):\n",
    "    dictionary = corpora.Dictionary(tokenized_data)\n",
    "    # dictionary.filter_extremes(no_below = 5, no_above = 500)  # Use it when the frequency is less than or more than n.\n",
    "    dictionary.save('common(DTM)_dict')\n",
    "    print(dictionary)\n",
    "else:\n",
    "    dictionary = Dictionary.load('common(DTM)_dict')\n",
    "\n",
    "# Construct corpus.\n",
    "if not os.path.exists('common(DTM)_corpus'):\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tokenized_data]\n",
    "    corpora.BleiCorpus.serialize('common(DTM)_corpus', corpus)\n",
    "else:\n",
    "    corpus = bleicorpus.BleiCorpus('common(DTM)_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary at a glance\n",
    "print(dictionary)\n",
    "for idx in dictionary:\n",
    "    print(dictionary[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus at a glance\n",
    "# 사람이 이해할 수 있는 형태로 코퍼스 사전 재구성 해보기 (term-frequency)\n",
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting the range for the hypothetical number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 6; end = 10; step = 1; # \"end\" is not included..\n",
    "\n",
    "passes = 50\n",
    "\n",
    "# Define one of inputs,times, to be consistent with ime_slice.\n",
    "import numpy as np\n",
    "times = np.arange(len(time_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the class for running Topic model.\n",
    "from gensim.models import ldaseqmodel\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Dynamic Topic Model while calculating coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classes to calculate coherence scores.\n",
    "!pip install tqdm\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.matutils import hellinger\n",
    "from tqdm import tqdm_notebook\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the functions to calculate DTM and coherence scores\n",
    "def compute_coherence(dictionary, corpus, passes, texts, times, start, end, step):\n",
    "    coherence_score_list = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm_notebook(range(start, end, step)):\n",
    "        ###################\n",
    "        start_dtm = time()\n",
    "        \n",
    "        dtm_name = \"common(DTM)_\" + str(num_topics)\n",
    "        \n",
    "        # Save the outcome at every calculation of each num_topics.\n",
    "        if os.path.exists(dtm_name):\n",
    "            dtm_model = ldaseqmodel.LdaSeqModel.load(dtm_name)\n",
    "        else:\n",
    "            dtm_model = ldaseqmodel.LdaSeqModel(corpus = corpus, id2word = dictionary,\n",
    "                                                time_slice = time_slice, num_topics = num_topics, passes = passes)\n",
    "            dtm_model.save(dtm_name)\n",
    "            \n",
    "        end_dtm = time()\n",
    "        ###################\n",
    "        print(\"Elapsed Time for DTM in %d topics: %.2f sec.\" % (num_topics, (end_dtm - start_dtm)))\n",
    "\n",
    "        topic_cs_list = []\n",
    "        for time_slot in range(len(times)):\n",
    "            topics_dtm = dtm_model.dtm_coherence(time = time_slot)\n",
    "            print(topics_dtm)\n",
    "            cs = CoherenceModel(topics = topics_dtm, texts = texts,\n",
    "                                dictionary = dictionary, coherence = 'c_v')\n",
    "            print(cs)\n",
    "            print(cs.get_coherence())\n",
    "            topic_cs_list.append(cs.get_coherence())\n",
    "            \n",
    "        model_list.append(dtm_model)\n",
    "        coherence_score_list.append(topic_cs_list)\n",
    "        \n",
    "    return model_list, coherence_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running Dynamic Topic Model while calculating coherence scores\n",
    "model_list, coherence_scores = compute_coherence(dictionary = dictionary, \n",
    "                                                 corpus = corpus,\n",
    "                                                 passes = passes,\n",
    "                                                 texts = tokenized_data,\n",
    "                                                 times = times,\n",
    "                                                 start = start, end = end, step = step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outcome\n",
    "import pickle\n",
    "\n",
    "# Save the outcome of the model for each num_topics\n",
    "with open(\"common(DTM)_ml.pk\", 'wb') as f:\n",
    "    pickle.dump(model_list, f)\n",
    "\n",
    "# Save the coherence scores for each num_topics\n",
    "with open(\"common(DTM)_cs.pk\", 'wb') as f:\n",
    "    pickle.dump(coherence_scores, f)\n",
    "    \n",
    "# #  Load the previously saved outcome\n",
    "# import pickle\n",
    "\n",
    "# # Load the previously saved outcome of the model for each num_topics\n",
    "# with open(\"tech(DTM)_ml.pk\", 'rb') as f:\n",
    "#     model_list = pickle.load(f)\n",
    "    \n",
    "# # Load the previously saved coherence scores for each num_topics\n",
    "# with open(\"tech(DTM)_cs.pk\", 'rb') as f:\n",
    "#     coherence_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Determine the optimal number of topics based on coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model_list and coherence scores\n",
    "print(model_list, coherence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the coherence scores\n",
    "print(coherence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For users' convenience, convert the model_list and coherence_scores into numpy array.\n",
    "import numpy as np\n",
    "\n",
    "np_coherence_scores = np.array(coherence_scores) \n",
    "np_cs_avg_list = np_coherence_scores.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the coherence scores declared as numpy array\n",
    "np_cs_avg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence Scores Graph\n",
    "# Set the fonts in Korean to print the coherence scores graph.\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family = 'AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family = font_name)\n",
    "elif platform.system() == 'Linux':\n",
    "    path = '/usr/share/fonts/truetype/malgun/malgun.ttf'\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family = font_name)\n",
    "else:\n",
    "    print('Unknown system... sorry~~~~')\n",
    "    \n",
    "x = range(start, end, step)\n",
    "\n",
    "plt.figure(figsize = (12, 10)) \n",
    "plt.plot(x, np_cs_avg_list, '-b')\n",
    "plt.xlabel(\"# of Topics\")\n",
    "plt.ylabel(\"AVG of Coherence Scores\")\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal number of coherence scores\n",
    "dtm_model = model_list[np.argmax(np_cs_avg_list)]\n",
    "len(dtm_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outcome of the applied optimal number of CS: The lists of the model at a certain period\n",
    "print(dtm_model.print_topics(time = 22, top_terms = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outcome of the applied optimal number of CS: The lists of the model for a certain topic\n",
    "dtm_model.print_topic_times(topic = 1, top_terms = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculating the probability where news would belong to the n-th topic for each period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the temporal information to indicate the time in which the news was released\n",
    "doc_time = []\n",
    "\n",
    "for i in range(len(time_slice)):\n",
    "    \n",
    "    for doc_id in range(len(tokenized_data)):\n",
    "        \n",
    "        if doc_id < time_slice[i]:\n",
    "            doc_time.append(i)\n",
    "            \n",
    "print(doc_time)\n",
    "print(len(doc_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_sum(arr, a):\n",
    "    arr = [0] + arr\n",
    "    partial_sum = [0] * len(arr)\n",
    "    \n",
    "    for i in range(1, len(arr)):\n",
    "        partial_sum[i] = partial_sum[i-1] + arr[i]\n",
    "        \n",
    "    partial_sum = partial_sum[1:]\n",
    "    # print(\"partial_sum\", partial_sum)\n",
    "    \n",
    "    # print(\"total sum\", partial_sum[-1])\n",
    "    \n",
    "    return partial_sum[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Select a certain sample size for each time_slot\n",
    "sample_idx = []\n",
    "sample_size = 500\n",
    "\n",
    "for i in range(len(time_slice)):\n",
    "    if i == 0:\n",
    "        item = np.random.randint(i, cumulative_sum(time_slice, i), size = sample_size)\n",
    "    else:\n",
    "        item = np.random.randint(cumulative_sum(time_slice, i-1), cumulative_sum(time_slice, i), size = sample_size)\n",
    "    \n",
    "    sample_idx.append(item)\n",
    "    \n",
    "print(sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_docs = []\n",
    "processing_time = []\n",
    "\n",
    "sample_indices = np.concatenate(([sample for sample in sample_idx]))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    processing_docs.append(tokenized_data[idx])\n",
    "    processing_time.append(doc_time[idx])\n",
    "    \n",
    "print(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(processing_docs))\n",
    "processing_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(processing_time))\n",
    "processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probabilities where each document belongs to an individual topic for each period.\n",
    "doc_dist = [] \n",
    "\n",
    "# dtm_model.doc_topics returns the probability where a document would belong to a specific topic.\n",
    "for doc_id in sample_indices:\n",
    "    doc_dist.append(dtm_model.doc_topics(doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing doc_dist to shaping it into DataFrame\n",
    "doc_dist = np.array(doc_dist)\n",
    "doc_topic_dist = doc_dist.T # Transpose\n",
    "doc_topic_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = dtm_model.num_topics\n",
    "\n",
    "for i in range(NUM_TOPICS):\n",
    "    print(\"%f\" % doc_dist[0][i], end = \", \")\n",
    "\n",
    "print()\n",
    "\n",
    "for i in range(NUM_TOPICS):\n",
    "    print(\"%f\" % doc_topic_dist[i][0], end = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dist[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shaping DataFrame (Users have to write the codes below to be consistent with the optimal number of topics.)\n",
    "import pandas as pd\n",
    "\n",
    "dtm_df = pd.DataFrame({\"Time\" : processing_time, \"Topic0\" : doc_topic_dist[0], \"Topic1\" : doc_topic_dist[1],\n",
    "                      \"Topic2\" : doc_topic_dist[2], \"Topic3\" : doc_topic_dist[3], \"Topic4\" : doc_topic_dist[4],\n",
    "                      \"Topic5\" : doc_topic_dist[5], \"Topic6\" : doc_topic_dist[6]})\n",
    "\n",
    "print(dtm_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate groupby object.\n",
    "doctopic_timeslot = dtm_df.groupby('Time')\n",
    "doctopic_timeslot.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the average probabilities for each time slot using mean() function.\n",
    "timeslot_avg = doctopic_timeslot.mean()\n",
    "timeslot_avg = timeslot_avg.reset_index()   # index reset\n",
    "timeslot_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum of the probabilties for each period is 1.\n",
    "timeslot_avg.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Use the codes below to be consistent with the optimal number of topics.\n",
    "plt.figure(figsize = (16, 12)) \n",
    "plt.plot(timeslot_avg['Time'], timeslot_avg['Topic0'], 'b*--', label = 'Topic 0')\n",
    "plt.plot(timeslot_avg['Time'], timeslot_avg['Topic1'], 'rs--', label = 'Topic 1')\n",
    "plt.plot(timeslot_avg['Time'], timeslot_avg['Topic2'], 'g^--', label = 'Topic 2')\n",
    "plt.plot(timeslot_avg['Time'], timeslot_avg['Topic3'], 'y*--', label = 'Topic 3')\n",
    "plt.plot(timeslot_avg['Time'], timeslot_avg['Topic4'], 'bs--', label = 'Topic 4')\n",
    "plt.plot(timeslot_avg['Time'], timeslot_avg['Topic5'], 'm*--', label = 'Topic 5')\n",
    "# plt.plot(timeslot_avg['Time'], timeslot_avg['Topic6'], 'k*--', label = 'Topic 6')\n",
    "# plt.plot(timeslot_avg['Time'], timeslot_avg['Topic7'], 'c*--', label = 'Topic 7')\n",
    "# plt.plot(timeslot_avg['Time'], timeslot_avg['Topic7'], 'c*--', label = 'Topic 8')\n",
    "# plt.plot(timeslot_avg['Time'], timeslot_avg['Topic7'], 'c*--', label = 'Topic 9')\n",
    "# plt.plot(timeslot_avg['Time'], timeslot_avg['Topic7'], 'c*--', label = 'Topic 10')\n",
    "# plt.plot(timeslot_avg['Time'], timeslot_avg['Topic7'], 'c*--', label = 'Topic 11')\n",
    "\n",
    "for topic_id in ['Topic0', 'Topic1', 'Topic2', 'Topic3', 'Topic4', 'Topic5', 'Topic6']:\n",
    "    for x, y in zip(range(len(time_slice)), timeslot_avg[topic_id]):\n",
    "        plt.annotate(\"%.4f\"%y, (x, y), textcoords = \"offset points\", xytext=(0,10), ha = 'center')\n",
    "\n",
    "plt.title(\"# News Topics\", fontsize = 14)\n",
    "plt.xlabel(\"Time\", fontsize = 13)\n",
    "plt.ylabel(\"The Portion of Each Topic (%)\", fontsize = 13)\n",
    "plt.xticks(timeslot_avg['Time'], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']) # 분석 대상 기간에 상응하게 조정\n",
    "plt.ylim([0.0, 0.25])\n",
    "plt.legend(loc = \"best\")\n",
    "plt.savefig(\"news_topic_distribution_graph.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
